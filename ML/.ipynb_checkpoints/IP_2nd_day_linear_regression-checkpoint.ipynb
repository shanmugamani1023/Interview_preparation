{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c819c0bb-256b-4b7e-ac06-768a729db459",
   "metadata": {},
   "source": [
    "# What is linear regreesion and its uses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a708f-2d3a-406d-aa05-6313e54700ce",
   "metadata": {},
   "source": [
    "regression used find relationship b/w dependent variable and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f695c5f-80a6-4769-9c6d-acd76915403f",
   "metadata": {},
   "source": [
    "# Regression:\n",
    "    Regression is a statistical method used in machine learning and statistics to examine the relationship between one dependent variable and one or more independent variables  The goal of regression analysis is to understand and quantify the relationship between these variables."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d89ede46-ef36-4ef1-aca2-1d693ceeb2f7",
   "metadata": {},
   "source": [
    "# linear regression, \n",
    "where the relationship between variables is assumed to be linear, and it will create best fit line using the relationship b/w dependent and independent features.\n",
    "ex: if x increses then y also increases.\n",
    "Hours Studied (X)  Exam Score (Y)\n",
    "-----------------  --------------\n",
    "      2                   60\n",
    "      3                   70\n",
    "      4                   75\n",
    "      5                   85\n",
    "      6                   90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efed69d-571e-4e80-a7aa-d78b81163a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b4d719e-5473-486d-9705-e6a93104f989",
   "metadata": {},
   "source": [
    "# Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73205537-0391-41a6-b133-de58b766e7e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are several types of linear regression models, each suited for different scenarios. Here are the main types:\n",
    "\n",
    "1. **Simple Linear Regression:**\n",
    "   - Involves only one independent variable.\n",
    "   - The relationship between the dependent variable and the independent variable is assumed to be linear.\n",
    "      - **Equation:** \\( Y = wx+b)\n",
    "   - **Example:** Using the hours studied (\\( X \\)) and exam scores (\\( Y \\)) example from before.\n",
    "   \n",
    "\n",
    "2. **Multiple Linear Regression:**\n",
    "   - Involves more than one independent variable.\n",
    "   - The relationship is modeled as a linear combination of the independent variables.\n",
    "   - **Equation:**=\n",
    "       y=(w1x1+w2x2+w3x3+...wnxn)+c\n",
    "   example:- **Example:** Predicting a person's salary (\\( Y \\)) based on multiple factors like years of experience (\\( X_1 \\)), education level (\\( X_2 \\)), and age (\\( X_3 \\)).\n",
    "\n",
    "3. **Polynomial Regression:**\n",
    "   - Extends the linear regression model by considering polynomial relationships.\n",
    "   - The relationship between the dependent variable and the independent variable is modeled as an nth degree polynomial.\n",
    "   - **Example:** Predicting house prices (\\( Y \\)) based on the square footage (\\( X \\)) using a polynomial regression to capture non-linear relationships.\n",
    "   Equation:\n",
    "       y=w1x1+w2x2^2+w3x3^3+...wnx^n+c\n",
    "       \n",
    "\n",
    "4. **Ridge Regression (L2 Regularization):**\n",
    "   - Adds a penalty term to the linear regression equation to prevent overfitting.\n",
    "   - Helps when there is multicollinearity among the independent variables.\n",
    "\n",
    "5. **Lasso Regression (L1 Regularization):**\n",
    "   - Similar to Ridge Regression but uses the absolute values of the coefficients.\n",
    "   - Can be useful for feature selection by driving some coefficients to exactly zero.\n",
    "\n",
    "6. **Elastic Net Regression:**\n",
    "   - Combines Ridge and Lasso regression.\n",
    "   - It has both L1 and L2 regularization terms.\n",
    "\n",
    "7. **Logistic Regression:**\n",
    "   - Despite its name, logistic regression is used for classification, not regression.\n",
    "   - It models the probability that an instance belongs to a particular category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8f7a1-6f80-41b0-9dea-6184cfbafd2b",
   "metadata": {},
   "source": [
    "# Diff b/w Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f4ed2-ceb4-4c58-9122-b2fcb672504f",
   "metadata": {},
   "source": [
    "Ridge regression and Lasso regression are both variations of linear regression that include regularization terms to prevent overfitting and handle multicollinearity. The main difference between the two lies in the type of regularization they apply:\n",
    "\n",
    "1. **Ridge Regression (L2 Regularization):**\n",
    "   - **Regularization Term:** \\( \\lambda \\sum_{i=1}^{n} b_i^2 \\)\n",
    "   - **Objective Function:** Minimize the sum of squared differences between predicted and actual values plus the regularization term.\n",
    "   - **Effect on Coefficients:** Ridge regression tends to shrink the coefficients toward zero, but it rarely sets them exactly to zero.\n",
    "   - **Use Case:** Ridge regression is useful when there is a high correlation among the independent variables, and you want to prevent multicollinearity.\n",
    "\n",
    "2. **Lasso Regression (L1 Regularization):**\n",
    "   - **Regularization Term:** \\( \\lambda \\sum_{i=1}^{n} |b_i| \\)\n",
    "   - **Objective Function:** Minimize the sum of squared differences between predicted and actual values plus the regularization term.\n",
    "   - **Effect on Coefficients:** Lasso regression not only shrinks the coefficients but also tends to set some of them exactly to zero.\n",
    "   - **Use Case:** Lasso regression is useful when you have a large number of features, and you want to perform feature selection by eliminating some of them.\n",
    "\n",
    "In summary, while both Ridge and Lasso regression introduce a penalty term to the linear regression objective function to prevent overfitting, Ridge tends to shrink coefficients towards zero without eliminating them, while Lasso can eliminate some coefficients entirely, effectively performing feature selection. The choice between the two depends on the specific characteristics of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e1869-1d25-4aae-b0b6-c7584ba0b29a",
   "metadata": {},
   "source": [
    "# how ridge regrssion can handle multicolinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fe528-dfde-4222-ae73-25374ee3f30e",
   "metadata": {},
   "source": [
    "Ridge regression is a regularization technique used in linear regression to handle multicollinearity, which occurs when two or more independent variables in a regression model are highly correlated. Multicollinearity can lead to unstable coefficient estimates and result in difficulties in interpreting the model.\n",
    "\n",
    "Ridge regression introduces a regularization term, also known as a penalty term, to the linear regression objective function. The objective function for ridge regression is:\n",
    "\n",
    "\\[ \\text{minimize } \\left\\{ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\} \\]\n",
    "\n",
    "Here:\n",
    "- \\( y_i \\) is the observed value for the \\(i\\)-th observation.\n",
    "- \\( \\beta_0 \\) is the intercept term.\n",
    "- \\( x_{ij} \\) is the \\(j\\)-th predictor variable for the \\(i\\)-th observation.\n",
    "- \\( \\beta_j \\) is the coefficient for the \\(j\\)-th predictor variable.\n",
    "- \\( \\lambda \\) is the regularization parameter that controls the strength of the penalty term.\n",
    "\n",
    "The additional term \\( \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\) penalizes large coefficient values. This penalty helps to shrink the coefficients toward zero, reducing their variance. As a result, ridge regression is particularly effective in mitigating multicollinearity by preventing the model from relying too heavily on any single variable.\n",
    "\n",
    "By introducing this penalty term, ridge regression improves the conditioning of the problem and provides more stable estimates of the coefficients, even in the presence of multicollinearity. The regularization term helps balance the trade-off between fitting the training data well and keeping the coefficients within reasonable bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701c492-e7fe-4530-87a6-ed004dd51093",
   "metadata": {},
   "source": [
    "# what is multicolinearity and example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e42958-1598-4ed2-b0b7-dd0ad5a129e4",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon in statistics and regression analysis where two or more independent variables in a regression model are highly correlated. This high correlation can cause issues in estimating the individual coefficients of the variables, leading to unstable and unreliable results. Multicollinearity does not affect the predictive power of the model, but it makes it challenging to interpret the significance of each variable.\n",
    "\n",
    "Here's a simple example to illustrate multicollinearity:\n",
    "\n",
    "Suppose you want to predict a person's income (\\( Y \\)) based on two independent variables: years of education (\\( X_1 \\)) and years of work experience (\\( X_2 \\)). A multicollinearity issue might arise if these two variables are strongly correlated.\n",
    "\n",
    "Example data:\n",
    "\n",
    "```\n",
    "| Person | Education (X1) | Experience (X2) | Income (Y) |\n",
    "|--------|----------------|------------------|------------|\n",
    "|   A    |       16       |        4         |   80,000   |\n",
    "|   B    |       18       |        6         |   90,000   |\n",
    "|   C    |       14       |        3         |   75,000   |\n",
    "|   D    |       16       |        5         |   85,000   |\n",
    "```\n",
    "\n",
    "In this example, if education and experience are highly correlated (for instance, people with more education tend to have more work experience), multicollinearity may occur. The issue arises when you try to estimate the coefficients of the regression equation:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\]\n",
    "\n",
    "High multicollinearity could make it difficult to determine the true effect of each variable on the outcome (income). The coefficients (\\( \\beta_1 \\) and \\( \\beta_2 \\)) might be unstable, and it becomes challenging to attribute changes in income to changes in either education or experience individually.\n",
    "\n",
    "To address multicollinearity, techniques like ridge regression or principal component analysis (PCA) can be employed to stabilize the coefficient estimates and improve the interpretability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea776838-2c76-47df-86d6-4dca2c318bbf",
   "metadata": {},
   "source": [
    "# How to find there is multicolinearity in dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ab0d375-e246-436f-8396-e381d032e041",
   "metadata": {
    "tags": []
   },
   "source": [
    "methods:\n",
    "    1.Correlation Matrix:\n",
    "    2.Correlation Coefficients:\n",
    "    3.Variance Inflation Factor (VIF):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabccb0f-6877-438a-aabe-f95b8f089c31",
   "metadata": {},
   "source": [
    "1.Co relationMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bffab6-95cb-4f07-95b5-8d981ff6c1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9229558f-7cf8-44ae-a49f-2343c008da02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908c5b1f-cafc-494f-8ec2-d4b417ab5fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data={\n",
    "\"x1\":[1,2,3,4,5],\n",
    "\"x2\":[2,4,6,8,10],\n",
    "\"y\":[3,9,12,15,17]\n",
    "}\n",
    "\n",
    "# we have a methods called corr in dataframes,so we have to create or convert our data into datafrane.\n",
    "df=pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f7939-b95d-463b-8871-7d505c38a415",
   "metadata": {},
   "source": [
    "# Mathmetical explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1917b-fce7-4238-82a2-73405732e1ba",
   "metadata": {},
   "source": [
    "# Progrmatical explanation for each and every type of regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83dc6c-0289-426b-ac7c-37322a441da3",
   "metadata": {},
   "source": [
    "# Real time example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe435eae-1e93-46f1-a21a-5ccb4bfddc44",
   "metadata": {},
   "source": [
    "# Evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6435ff-dbdd-4beb-bed1-a77176c69707",
   "metadata": {},
   "source": [
    "# Basic Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b06db-d26c-4cef-bd9a-7f12b63c23cb",
   "metadata": {},
   "source": [
    "# Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf0776-aae5-462f-907a-03fd596dc401",
   "metadata": {},
   "source": [
    "# Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e6a49-a5ab-4a12-8088-94b2d8df0654",
   "metadata": {},
   "source": [
    "# Is sensitive to outliers and how to handle? and coding part ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fb752-6835-4964-8673-fb7ea70813db",
   "metadata": {},
   "source": [
    "# what will happen when it contains missing values and how to handle? coding part?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eef718-91b3-4b1d-804d-2a13ab9d028c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature scaling required? and what is feature scaling and types and implematation of feature scaling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
